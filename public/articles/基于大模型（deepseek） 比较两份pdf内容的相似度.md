å¯ä»¥ï¼ŒåŸºäºå¤§æ¨¡å‹ï¼ˆå¦‚ DeepSeekï¼‰æ¥æ¯”è¾ƒä¸¤ä»½ PDF å†…å®¹çš„ç›¸ä¼¼åº¦ï¼Œä¸€èˆ¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°ï¼š

---

### âœ… æ€»ä½“æµç¨‹

1. **è§£æ PDF æ–‡æœ¬**
   - ä½¿ç”¨å·¥å…·ï¼ˆå¦‚ `PyMuPDF`, `pdfplumber`, `pdfminer`ï¼‰æå–ä¸¤ä»½ PDF çš„æ–‡æœ¬å†…å®¹ã€‚

2. **æ¸…æ´—æ–‡æœ¬**
   - å»é™¤é¡µçœ‰é¡µè„šã€ç©ºè¡Œã€ç‰¹æ®Šå­—ç¬¦ï¼Œæ ‡å‡†åŒ–æ ¼å¼ã€‚

3. **å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºå°æ®µ**
   - å› ä¸ºå¤§æ¨¡å‹é€šå¸¸æœ‰ token é™åˆ¶ï¼Œéœ€å°†é•¿æ–‡åˆ‡åˆ†æˆè‹¥å¹²æ®µè½ï¼ˆå¦‚æ¯æ®µ 300-500 ä¸ªè¯ï¼‰ã€‚

4. **å‘é‡åŒ–æ–‡æœ¬ï¼ˆembeddingï¼‰**
   - ä½¿ç”¨ DeepSeek Embedding æ¨¡å‹æˆ–ç±»ä¼¼çš„ä¸­æ–‡è¯­ä¹‰å‘é‡æ¨¡å‹ï¼Œå°†æ¯æ®µæ–‡æœ¬è½¬æˆå‘é‡ã€‚

5. **è®¡ç®—ç›¸ä¼¼åº¦**
   - å¯¹ä¸¤ä»½æ–‡æ¡£çš„å‘é‡è¿›è¡Œä½™å¼¦ç›¸ä¼¼åº¦åŒ¹é…ï¼Œå¯ä»¥ç”¨å¹³å‡ç›¸ä¼¼åº¦ã€æœ€å¤§ç›¸ä¼¼åº¦ç­‰æ–¹æ³•è¿›è¡Œæ¯”å¯¹ã€‚

6. **è¾“å‡ºç›¸ä¼¼åº¦ç»“æœ**
   - æœ€åç»™å‡ºæ•´ä½“ç›¸ä¼¼åº¦è¯„åˆ†ï¼Œæˆ–è€…é«˜ç›¸ä¼¼æ®µè½çš„å…·ä½“å†…å®¹ã€‚

---

### ğŸ› ï¸ ç¤ºä¾‹æ–¹æ¡ˆï¼ˆåŸºäº DeepSeekï¼‰

å‡è®¾ä½ ç”¨ Python+API æ¥å£è°ƒç”¨ DeepSeekï¼š

#### 1. æå– PDF æ–‡æœ¬
```python
import fitz  # PyMuPDF

def extract_text_from_pdf(path):
    doc = fitz.open(path)
    return "\n".join([page.get_text() for page in doc])
```

#### 2. è°ƒç”¨ DeepSeek Embedding æ¥å£
```python
import requests

def get_embedding(text):
    response = requests.post(
        url="https://api.deepseek.com/embedding",
        headers={"Authorization": "Bearer YOUR_API_KEY"},
        json={"input": text}
    )
    return response.json()["data"][0]["embedding"]
```

#### 3. è®¡ç®—ç›¸ä¼¼åº¦
```python
from sklearn.metrics.pairwise import cosine_similarity

def cosine_sim(a, b):
    return cosine_similarity([a], [b])[0][0]
```

---

### ğŸ“Š è¾“å‡ºç»“æœç¤ºä¾‹

```text
æ–‡æ¡£A vs æ–‡æ¡£B ç›¸ä¼¼åº¦è¯„åˆ†: 0.87

é«˜åº¦ç›¸ä¼¼æ®µè½ï¼š
[æ–‡æ¡£A ç¬¬3æ®µè½] å’Œ [æ–‡æ¡£B ç¬¬2æ®µè½] ç›¸ä¼¼åº¦ä¸º 0.96
...
```

---

### ğŸ§  å¦‚æœä½ ä¸æƒ³è‡ªå·±å†™ä»£ç â€¦

æˆ‘ä¹Ÿå¯ä»¥å¸®ä½ åšè¿™ä¸ªæ¯”è¾ƒã€‚ä½ å¯ä»¥ï¼š
- ä¸Šä¼ ä½ è¦å¯¹æ¯”çš„ä¸¤ä¸ª PDF æ–‡ä»¶
- æˆ‘æ¥å¸®ä½ å¤„ç†å¹¶è¾“å‡ºç›¸ä¼¼åº¦ç»“æœï¼ˆä½¿ç”¨é€šç”¨å¼€æº embedding æ¨¡å‹ï¼Œæˆ–æ¨¡æ‹Ÿ DeepSeek æ•ˆæœï¼‰

---

ä½ æƒ³ç”¨ä»£ç å®ç°ã€ç”¨æˆ‘å¸®ä½ åˆ†æï¼Œè¿˜æ˜¯ç”¨ Web å·¥å…·æ¯”è¾ƒï¼Ÿå“ªä¸ªæ›´æ–¹ä¾¿ä½ ç°åœ¨æ“ä½œï¼Ÿ



æƒ³ç”¨ä»£ç å®ç° 




å¥½ï¼é‚£æˆ‘ä»¬å°±ä¸€æ­¥æ­¥æ¥æå®šã€‚

ä½ æåˆ°ä½¿ç”¨ **DeepSeek**ï¼Œè¿™é‡Œæˆ‘ç»™å‡ºä¸€ä¸ªå®Œæ•´çš„å¯è¿è¡Œ Python è„šæœ¬ç»“æ„ï¼Œé€‚ç”¨äºä½ åœ¨æœ¬åœ°éƒ¨ç½²æˆ–ç»“åˆ DeepSeek API ä½¿ç”¨ã€‚ä½ ä¹Ÿå¯ä»¥å…ˆè·‘é€šåŸºç¡€ç‰ˆï¼ˆç”¨å¼€æº Embedding æ¨¡å‹ï¼‰ï¼Œåé¢æ¢ DeepSeek æ¥å£ä¹Ÿå¾ˆæ–¹ä¾¿ã€‚

---

## âœ… å®ç°ç›®æ ‡

æˆ‘ä»¬å†™ä¸€ä¸ª Python è„šæœ¬ï¼Œå¯ä»¥ï¼š
1. è¯»å–ä¸¤ä¸ª PDF
2. æå–æ–‡æœ¬
3. åˆ‡åˆ†æˆæ®µè½
4. å‘é‡åŒ–ï¼ˆç”¨ `sentence-transformers` æˆ–æ¢æˆ DeepSeekï¼‰
5. è®¡ç®—ç›¸ä¼¼åº¦

---

## ğŸ§° æ‰€éœ€ç¯å¢ƒ

ä½ éœ€è¦å®‰è£…è¿™äº›åŒ…ï¼š

```bash
pip install PyMuPDF sentence-transformers scikit-learn numpy
```

---

## ğŸ§‘â€ğŸ’» Python è„šæœ¬ç¤ºä¾‹

```python
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# 1. æå– PDF æ–‡æœ¬
def extract_text_from_pdf(path):
    doc = fitz.open(path)
    text = "\n".join([page.get_text() for page in doc])
    return text

# 2. åˆ‡åˆ†ä¸ºæ®µè½ï¼ˆæ¯ 500 å­—åˆ‡ä¸€æ®µï¼‰
def split_text(text, max_length=500):
    return [text[i:i+max_length] for i in range(0, len(text), max_length)]

# 3. è·å–æ–‡æœ¬ embeddingï¼ˆå¯æ¢æˆ DeepSeekï¼‰
def get_embeddings(paragraphs, model):
    return model.encode(paragraphs)

# 4. è®¡ç®—ä¸¤ä¸ªæ–‡æ¡£çš„ç›¸ä¼¼åº¦ï¼ˆå¹³å‡æ®µè½ç›¸ä¼¼åº¦ï¼‰
def compute_similarity(emb1, emb2):
    sim_matrix = cosine_similarity(emb1, emb2)
    max_similarities = np.max(sim_matrix, axis=1)
    return np.mean(max_similarities)

# ä¸»æµç¨‹
def compare_pdfs(pdf_path1, pdf_path2):
    print("åŠ è½½ PDF...")
    text1 = extract_text_from_pdf(pdf_path1)
    text2 = extract_text_from_pdf(pdf_path2)

    print("åˆ†æ®µ...")
    paragraphs1 = split_text(text1)
    paragraphs2 = split_text(text2)

    print("åŠ è½½ embedding æ¨¡å‹ï¼ˆæˆ–æ›¿æ¢æˆ DeepSeek æ¥å£ï¼‰...")
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # æ”¯æŒä¸­æ–‡

    print("è®¡ç®—å‘é‡...")
    emb1 = get_embeddings(paragraphs1, model)
    emb2 = get_embeddings(paragraphs2, model)

    print("æ¯”è¾ƒç›¸ä¼¼åº¦...")
    score = compute_similarity(emb1, emb2)

    print(f"\nğŸ“Š æ–‡æ¡£ç›¸ä¼¼åº¦å¾—åˆ†ï¼š{score:.4f} ï¼ˆ1.0ä¸ºå®Œå…¨ç›¸ä¼¼ï¼‰")

# ç¤ºä¾‹ä½¿ç”¨
compare_pdfs('doc1.pdf', 'doc2.pdf')
```

---

## ğŸš€ å¯é€‰ï¼šæ›¿æ¢æˆ DeepSeek API è·å– Embedding

å¦‚æœä½ ä¸æƒ³ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥æŠŠ `get_embeddings()` æ”¹ä¸ºè°ƒç”¨ DeepSeek APIï¼š

```python
import requests

def get_embeddings_from_deepseek(paragraphs, api_key):
    embeddings = []
    for text in paragraphs:
        response = requests.post(
            "https://api.deepseek.com/embedding",
            headers={"Authorization": f"Bearer {api_key}"},
            json={"input": text}
        )
        vec = response.json()["data"][0]["embedding"]
        embeddings.append(vec)
    return embeddings
```

---

ä½ å¯ä»¥å…ˆè¯•è¯•è¿™ä¸ªä»£ç è·‘å¾—é¡ºä¸é¡ºï¼Œå¦‚æœä½ å·²ç»æœ‰ PDF æ–‡ä»¶äº†ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘æ–‡ä»¶å†…å®¹ï¼ˆæˆ–ä¸Šä¼ ï¼‰ï¼Œæˆ‘å¯ä»¥å¸®ä½ æµ‹ä¸€ä»½åšå‚è€ƒã€‚

æ˜¯å¦éœ€è¦æˆ‘æ‰“åŒ…æˆ `.py` æ–‡ä»¶æ–¹ä¾¿ä½ ç›´æ¥è¿è¡Œï¼Ÿæˆ–è€…ä½ å¸Œæœ›ç”¨ Java æ¥è°ƒ Python è„šæœ¬ï¼Ÿ



å¦‚æœpdf ä¸­åŒ…å« å›¾ç‰‡ è¿™ç§åº”è¯¥å¦‚ä½•å¤„ç†




éå¸¸å¥½çš„é—®é¢˜ï¼

PDF ä¸­åŒ…å«**å›¾ç‰‡ï¼ˆæ¯”å¦‚å›¾è¡¨ã€æˆªå›¾ã€æ‰«æç‰ˆ PDFï¼‰**çš„æƒ…å†µæ˜¯éå¸¸å¸¸è§çš„ï¼Œè€Œä¸”å®ƒä¸èƒ½ç›´æ¥é€šè¿‡ `page.get_text()` æå–å‡ºæœ‰ç”¨æ–‡å­—å†…å®¹ã€‚

---

## âœ… æ ¹æ®å†…å®¹ç±»å‹ï¼Œæœ‰ä¸¤ç§å¤„ç†ç­–ç•¥ï¼š

### ğŸ”¹ **1. å¯æœç´¢æ–‡å­—çš„ PDFï¼ˆæ­£å¸¸ PDFï¼‰**
- ç”¨ `PyMuPDF` æˆ– `pdfplumber` æå–æ–‡å­—ï¼Œæ²¡é—®é¢˜ã€‚
- **ä¸éœ€è¦é¢å¤–å¤„ç†ã€‚**

### ğŸ”¹ **2. å›¾åƒå‹ PDFï¼ˆæ‰«æä»¶ã€å›¾ç‰‡ä¸ºä¸»ï¼‰**
- è¿™ç±» PDF çœ‹èµ·æ¥æœ‰â€œæ–‡å­—â€ï¼Œä½†å®é™…ä¸Šæ˜¯å›¾ç‰‡ã€‚
- éœ€è¦ç”¨ **OCRï¼ˆæ–‡å­—è¯†åˆ«ï¼‰** æŠ€æœ¯å¤„ç†ï¼Œæ¯”å¦‚ï¼š
  - PaddleOCRï¼ˆå¼€æºã€æ”¯æŒä¸­æ–‡ï¼‰
  - Tesseract OCRï¼ˆå›½é™…åŒ–å¼ºï¼‰
  - ç™¾åº¦æ–‡å­—è¯†åˆ«ã€é˜¿é‡Œäº‘ OCR æ¥å£ï¼ˆä»˜è´¹ï¼‰

---

## ğŸ§° æ¨èæ–¹æ¡ˆï¼šç”¨ PaddleOCR æå– PDF ä¸­çš„å›¾åƒæ–‡å­—

ä½ å¯ä»¥è¿™æ ·åšï¼š

### æ­¥éª¤ï¼š

1. **ä» PDF ä¸­æå–æ¯ä¸€é¡µå›¾åƒ**
2. **ä½¿ç”¨ OCR æ¨¡å‹è¯†åˆ«å›¾åƒä¸­çš„æ–‡å­—**
3. **å°†è¯†åˆ«ç»“æœæ‹¼æ¥æˆå®Œæ•´æ–‡æ¡£æ–‡æœ¬**

---

## ğŸ§‘â€ğŸ’» ç¤ºä¾‹ä»£ç ï¼ˆæå–å›¾ç‰‡ + OCRï¼‰

å®‰è£…ä¾èµ–ï¼š

```bash
pip install paddleocr pdf2image paddlepaddle
sudo apt install poppler-utils  # Linuxç³»ç»Ÿå¿…é¡»ï¼ˆWindows/Macéœ€é¢å¤–é…ç½®ï¼‰
```

Python ä»£ç ï¼š

```python
from pdf2image import convert_from_path
from paddleocr import PaddleOCR
import os

# 1. ä» PDF æå–å›¾ç‰‡
def pdf_to_images(pdf_path):
    images = convert_from_path(pdf_path, dpi=300)
    return images

# 2. OCR æå–æ–‡å­—ï¼ˆä¸­æ–‡æ”¯æŒï¼‰
def ocr_images(images):
    ocr = PaddleOCR(use_angle_cls=True, lang='ch')  # ä¸­æ–‡æ¨¡å‹
    all_text = ""
    for idx, img in enumerate(images):
        print(f"æ­£åœ¨è¯†åˆ«ç¬¬ {idx + 1} é¡µ...")
        result = ocr.ocr(np.array(img), cls=True)
        for line in result:
            for word_info in line:
                all_text += word_info[1][0] + "\n"
    return all_text

# æ€»æµç¨‹
def extract_text_from_scanned_pdf(pdf_path):
    images = pdf_to_images(pdf_path)
    text = ocr_images(images)
    return text
```

---

## ğŸ” æœ€ç»ˆèåˆå»ºè®®

ä½ å¯ä»¥è¿™æ ·å†™ä¸€ä¸ªæ™ºèƒ½å‡½æ•°ï¼š

```python
def smart_extract_pdf_text(pdf_path, fallback_ocr=False):
    text = extract_text_from_pdf(pdf_path)
    if len(text.strip()) < 100 and fallback_ocr:  # å†…å®¹å¤ªå°‘ï¼Œå¯èƒ½æ˜¯å›¾ç‰‡å‹ PDF
        print("å†…å®¹ä¸ºç©ºæˆ–å¾ˆå°‘ï¼Œä½¿ç”¨ OCR æå–...")
        text = extract_text_from_scanned_pdf(pdf_path)
    return text
```

---

## ğŸš¦æ€»ç»“

| PDF ç±»å‹         | å¤„ç†æ–¹å¼              |
|------------------|-----------------------|
| æ­£å¸¸ PDFï¼ˆå«æ–‡æœ¬ï¼‰ | ç›´æ¥ç”¨ `PyMuPDF` æå– |
| æ‰«æ PDFï¼ˆå›¾åƒï¼‰   | ç”¨ PaddleOCR æå–æ–‡å­— |
| æ··åˆå‹ PDF        | å…ˆè¯•æ–‡æœ¬æå–ï¼Œå¿…è¦æ—¶ç”¨ OCR |

---

å¦‚æœä½ æä¾›ä¸€ä»½ç¤ºä¾‹ PDFï¼Œæˆ‘å¯ä»¥å¸®ä½ åˆ¤æ–­æ˜¯å“ªç§ç±»å‹ï¼Œå¹¶ç»™ä½ å®šåˆ¶ä¸€æ®µè„šæœ¬ã€‚å¦‚æœä½ åªæƒ³æå–ä¸€é¡µæ¥è¯•æ•ˆæœï¼Œä¹Ÿè¡Œ ğŸ˜

ä½ æ˜¯æƒ³å¤„ç†å“ªä¸€ç±» PDFï¼Ÿæˆ‘å¯ä»¥æŒ‰ä½ çš„ä½¿ç”¨åœºæ™¯ç»™ä½ ä¼˜åŒ–æ–¹æ¡ˆã€‚

æ··åˆå‹ PDF	



ğŸ‘Œæ˜ç™½äº†ï¼Œä½ è¦å¤„ç†çš„æ˜¯ **æ··åˆå‹ PDF**ï¼šä¸€éƒ¨åˆ†é¡µé¢æ˜¯æ­£å¸¸çš„å¯é€‰ä¸­æ–‡æœ¬ï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯å›¾åƒï¼ˆæ‰«æé¡µã€å›¾è¡¨é¡µç­‰ï¼‰å½¢å¼ã€‚

è¿™ç±» PDF è¦**â€œæ™ºèƒ½æå–æ–‡å­—â€**ï¼Œéœ€è¦æ ¹æ®æ¯é¡µæƒ…å†µæ¥åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ OCRã€‚è¿™ç§æ™ºèƒ½æµç¨‹ï¼Œæˆ‘æ¥å¸®ä½ æ­ä¸€ä¸ª âœ¨**è‡ªåŠ¨åˆ¤æ–­ + ä¸¤ç§æå–æ–¹å¼ç»“åˆ**âœ¨ çš„å®Œæ•´æ–¹æ¡ˆã€‚

---

## âœ… æ··åˆå‹ PDF æ™ºèƒ½å¤„ç†é€»è¾‘

æˆ‘ä»¬æŒ‰ä»¥ä¸‹æµç¨‹æ¥åšï¼š

1. **éå†æ¯ä¸€é¡µ**
2. ä¼˜å…ˆå°è¯•æå–æ–‡å­—ï¼ˆç”¨ PyMuPDFï¼‰
3. å¦‚æœæ–‡å­—å¾ˆå°‘ï¼ˆå¯èƒ½æ˜¯å›¾åƒï¼‰ï¼Œè‡ªåŠ¨è½¬å›¾åƒå¹¶ç”¨ OCRï¼ˆPaddleOCRï¼‰
4. æ‹¼æ¥æˆå®Œæ•´æ–‡æœ¬ï¼Œè¿”å›ç»™åç»­ç›¸ä¼¼åº¦åˆ†ææµç¨‹

---

## ğŸ§‘â€ğŸ’» Python å®Œæ•´ç¤ºä¾‹ï¼šæ··åˆ PDF æ™ºèƒ½æå–æ–‡å­—

> ğŸ’¡ å‰æï¼šä½ å·²ç»å®‰è£…å¥½ `pymupdf`, `pdf2image`, `paddleocr`, `paddlepaddle`, `poppler-utils`

```python
import fitz  # PyMuPDF
from pdf2image import convert_from_path
from paddleocr import PaddleOCR
import numpy as np

# åˆå§‹åŒ– OCR å¼•æ“
ocr = PaddleOCR(use_angle_cls=True, lang='ch')

# æå–ä¸€é¡µå›¾ç‰‡æ–‡å­—
def ocr_page_image(pdf_path, page_num):
    images = convert_from_path(pdf_path, dpi=300, first_page=page_num + 1, last_page=page_num + 1)
    img = np.array(images[0])
    result = ocr.ocr(img, cls=True)
    text = ""
    for line in result:
        for word_info in line:
            text += word_info[1][0] + "\n"
    return text

# æ··åˆå‹ PDF æ™ºèƒ½æå–
def extract_text_from_mixed_pdf(pdf_path, min_text_len=50):
    doc = fitz.open(pdf_path)
    all_text = ""
    for i, page in enumerate(doc):
        print(f"å¤„ç†ç¬¬ {i+1} é¡µ...")
        text = page.get_text().strip()
        if len(text) >= min_text_len:
            all_text += text + "\n"
        else:
            print(f"ç¬¬ {i+1} é¡µæ–‡å­—è¾ƒå°‘ï¼Œå°è¯• OCR è¯†åˆ«...")
            ocr_text = ocr_page_image(pdf_path, i)
            all_text += ocr_text + "\n"
    return all_text
```

---

## âœ¨ é›†æˆåˆ°ç›¸ä¼¼åº¦å¯¹æ¯”æµç¨‹é‡Œ

æŠŠ `extract_text_from_mixed_pdf()` æ›¿æ¢ä½ åŸæ¥æå– PDF æ–‡æœ¬çš„å‡½æ•°å³å¯ï¼š

```python
text1 = extract_text_from_mixed_pdf('doc1.pdf')
text2 = extract_text_from_mixed_pdf('doc2.pdf')
```

ç„¶åç»§ç»­èµ°åˆ†æ®µ â†’ å‘é‡åŒ– â†’ ç›¸ä¼¼åº¦æ¯”å¯¹çš„æµç¨‹ã€‚

---

## ğŸ“¦ æ–‡ä»¶ç»“æ„å»ºè®®

```
compare_pdf_similarity/
â”œâ”€â”€ compare.py                # ä¸»æµç¨‹è„šæœ¬
â”œâ”€â”€ pdf_utils.py              # æå–æ–‡å­—ç”¨çš„æ¨¡å—ï¼ˆä¸Šé¢ä»£ç ï¼‰
â”œâ”€â”€ doc1.pdf
â”œâ”€â”€ doc2.pdf
â””â”€â”€ requirements.txt
```

---

## ğŸ§ª ä½ å¯ä»¥å…ˆè¯•è¯•ï¼š

- ç”¨å‡ é¡µåŒ…å«å›¾åƒå’Œæ–‡å­—çš„ PDF è·‘è·‘
- æ‰“å°æå–å‡ºæ¥çš„ `text1[:1000]` çœ‹å®é™…å†…å®¹
- çœ‹ PaddleOCR æå–çš„æ–‡å­—æ˜¯å¦å‡†ç¡®ï¼Œæ˜¯å¦è¦è°ƒ OCR DPIï¼ˆé»˜è®¤ 300ï¼‰

---

å¦‚æœä½ æƒ³æˆ‘å¸®ä½ é›†æˆæˆä¸€ä¸ªå®Œæ•´è„šæœ¬ï¼ˆå«ç›¸ä¼¼åº¦åˆ†æï¼‰ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç›´æ¥æ‰“åŒ…ã€‚ä½ ä¹Ÿå¯ä»¥æŠŠä½ ä¸¤ä»½ PDF å‘ç»™æˆ‘çœ‹çœ‹ï¼Œæˆ‘æµ‹ä¸€æµ‹ç›¸ä¼¼åº¦ç»™ä½ å‚è€ƒã€‚

ç»§ç»­ä¼˜åŒ–ä¸‹å»çš„è¯ï¼Œç”šè‡³è¿˜èƒ½æŠŠå›¾åƒä¿å­˜ã€ç›¸ä¼¼åº¦é«˜çš„æ®µè½å®šä½åˆ°é¡µç  ğŸ˜

ä½ æƒ³ç»§ç»­æ·±åº¦å®šåˆ¶ä¸€ä¸‹è„šæœ¬ï¼Œè¿˜æ˜¯å…ˆæµ‹é€šç”¨ç‰ˆæœ¬ï¼Ÿ


æŠŠå›¾åƒä¿å­˜ã€ç›¸ä¼¼åº¦é«˜çš„æ®µè½å®šä½åˆ°é¡µç 
